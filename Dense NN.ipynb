{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cbf943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9e9ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  11 of 11 completed\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11022 entries, 0 to 11021\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype              \n",
      "---  ------          --------------  -----              \n",
      " 0   date            11022 non-null  datetime64[ns, UTC]\n",
      " 1   ticker          11022 non-null  object             \n",
      " 2   value           11022 non-null  float64            \n",
      " 3   log_diff        11022 non-null  float64            \n",
      " 4   moving_avg      11022 non-null  float64            \n",
      " 5   sig_delta_up    11022 non-null  int64              \n",
      " 6   sig_delta_down  11022 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), float64(3), int64(2), object(1)\n",
      "memory usage: 602.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2755 entries, 0 to 2754\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype              \n",
      "---  ------          --------------  -----              \n",
      " 0   date            2755 non-null   datetime64[ns, UTC]\n",
      " 1   ticker          2755 non-null   object             \n",
      " 2   value           2755 non-null   float64            \n",
      " 3   log_diff        2755 non-null   float64            \n",
      " 4   moving_avg      2755 non-null   float64            \n",
      " 5   sig_delta_up    2755 non-null   int64              \n",
      " 6   sig_delta_down  2755 non-null   int64              \n",
      "dtypes: datetime64[ns, UTC](1), float64(3), int64(2), object(1)\n",
      "memory usage: 150.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = yf.download(\n",
    "    \"GOOG AAPL MSFT POLX.L POW.L AMD TSLA F KR UMC GM\",\n",
    "    start=\"2018-01-01\",\n",
    "    end=\"2023-01-01\",\n",
    "    group_by=\"ticker\",\n",
    ")\n",
    "break_date = dt.date(2022, 1, 1)\n",
    "\n",
    "# Reshape data\n",
    "data = (\n",
    "    data.unstack()\n",
    "    .reset_index()\n",
    "    .rename(\n",
    "        columns={\"Date\": \"date\", \"level_0\": \"ticker\", \"level_1\": \"measure\", 0: \"value\"}\n",
    "    )\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Filter to adj close\n",
    "data = data.loc[\n",
    "    data[\"measure\"] == \"Adj Close\", [\"date\", \"ticker\", \"value\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Calculate log diff\n",
    "data[\"log_diff\"] = data.groupby(\"ticker\")[\"value\"].transform(\n",
    "    lambda x: np.log(x) - np.log(x.shift())\n",
    ")\n",
    "\n",
    "# Split timeseries now to prevent leakage.\n",
    "data_train = data.loc[data[\"date\"].dt.date < break_date, :].copy()\n",
    "data_test = data.loc[~data.index.isin(data_train.index), :].copy()\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Moving average\n",
    "    data[\"moving_avg\"] = data[\"value\"].rolling(5).mean()\n",
    "    \n",
    "    # Add flag for whether the value is up in the next weeks moving average is up 10 % or down 10%\n",
    "    data[\"sig_delta_up\"] = data.groupby(\"ticker\")[\"value\"].transform(\n",
    "        lambda x: np.where(((x.shift(-5) - x) / x) > 0.1, 1, 0)\n",
    "    )\n",
    "    data[\"sig_delta_down\"] = data.groupby(\"ticker\")[\"value\"].transform(\n",
    "        lambda x: np.where(((x.shift(-5) - x) / x) < -0.1, 1, 0)\n",
    "    )\n",
    "    # Remove nas and reset the index\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "data_train, data_test = map(preprocess_data, [data_train, data_test])\n",
    "\n",
    "data_train.info(), data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb0f96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11022, 1), (11022, 2), (2755, 1), (2755, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unpack_data(data):\n",
    "    return {\n",
    "        ticker: {\n",
    "            \"log_diff\": data[(data[\"ticker\"] == ticker)][\"log_diff\"].values.reshape(\n",
    "                -1, 1\n",
    "            ),\n",
    "            \"y\": data.loc[\n",
    "                (data[\"ticker\"] == ticker), [\"sig_delta_up\", \"sig_delta_down\"]\n",
    "            ].values,\n",
    "        }\n",
    "        for ticker in data[\"ticker\"].unique()\n",
    "    }\n",
    "\n",
    "\n",
    "def get_scale_models(array_train):\n",
    "    return {\n",
    "        ticker: MinMaxScaler().fit(array_train[ticker][\"log_diff\"])\n",
    "        for ticker in array_train.keys()\n",
    "    }\n",
    "\n",
    "\n",
    "def scale_array(array, scalers):\n",
    "    for ticker in array.keys():\n",
    "        array[ticker][\"log_diff\"] = scalers[ticker].transform(array[ticker][\"log_diff\"])\n",
    "    return array\n",
    "\n",
    "\n",
    "def stack_arrays(array):\n",
    "    x = np.vstack([val[\"log_diff\"] for val in array.values()])\n",
    "    y = np.vstack([val[\"y\"] for val in array.values()])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "array_train, array_test = map(unpack_data, [data_train, data_test])\n",
    "\n",
    "scalers = get_scale_models(array_train)\n",
    "\n",
    "array_train, array_test = [\n",
    "    scale_array(array, scalers) for array in [array_train, array_test]\n",
    "]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = map(stack_arrays, [array_train, array_test])\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85d8783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([11011, 10]),\n",
       " torch.Size([11011, 2]),\n",
       " torch.Size([2741, 10]),\n",
       " torch.Size([2741, 2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookback = 10\n",
    "\n",
    "\n",
    "def vectorized_stride(array, sub_window_size=lookback, stride_size=1):\n",
    "    max_time = array.shape[0]\n",
    "    start = max_time % sub_window_size\n",
    "    sub_windows = (\n",
    "        start\n",
    "        + np.expand_dims(np.arange(sub_window_size), 0)\n",
    "        + np.expand_dims(\n",
    "            np.arange(max_time + 1 - (start + sub_window_size), step=stride_size), 0\n",
    "        ).T\n",
    "    )\n",
    "    return array[sub_windows]\n",
    "\n",
    "\n",
    "def trim_y(array, sub_window_size=lookback):\n",
    "    max_time = array.shape[0]\n",
    "    start = max_time % sub_window_size + sub_window_size - 1\n",
    "    print(start)\n",
    "    return array[start:]\n",
    "\n",
    "\n",
    "X_train = vectorized_stride(x_train).reshape((-1, lookback)).astype(\"float32\")\n",
    "X_test = vectorized_stride(x_test).reshape((-1, lookback)).astype(\"float32\")\n",
    "y_train = trim_y(y_train).astype(\"float32\")\n",
    "y_test = trim_y(y_test).astype(\"float32\")\n",
    "\n",
    "X_train, y_train, X_test, y_test = [\n",
    "    torch.from_numpy(array) for array in [X_train, y_train, X_test, y_test]\n",
    "]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383c0d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.8, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=500, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.8, inplace=False)\n",
       "    (6): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.8, inplace=False)\n",
       "    (9): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.8, inplace=False)\n",
       "    (12): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.8, inplace=False)\n",
       "    (15): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (16): ReLU()\n",
       "    (17): Dropout(p=0.8, inplace=False)\n",
       "    (18): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Dropout(p=0.8, inplace=False)\n",
       "    (21): Linear(in_features=100, out_features=2, bias=True)\n",
       "    (22): ReLU()\n",
       "    (23): Dropout(p=0.8, inplace=False)\n",
       "    (24): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(10, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(100, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(500, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(100, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70aae659",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "\n",
    "def train(X, y, model, size=None, batch_size=1000):\n",
    "    # X is a torch Variable\n",
    "    permutation = torch.randperm(X.size()[0])\n",
    "\n",
    "    for i in range(0, X.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i : i + batch_size]\n",
    "        batch_x, batch_y = X[indices], y[indices]\n",
    "\n",
    "        # in case you wanted a semi-full example\n",
    "        outputs = model.forward(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss, current = loss.item(), i * len(X)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(X, y, model):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X)):\n",
    "            pred = model(X[i])\n",
    "            test_loss += loss_fn(pred, y[i]).item()\n",
    "            correct += (pred.round() == y[i]).type(torch.float).sum().item()\n",
    "    #             if i == 1:\n",
    "    #                 print(pred.round(), y[i])\n",
    "    test_loss /= len(X)\n",
    "    correct /= y.numel()\n",
    "    return (100 * correct), test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-------------0\n",
      "loss: -0.000000\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.079658 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1001):\n",
    "    loss = train(X_train, y_train, net, len(X_train))\n",
    "    accuracy, test_loss = test(X_test, y_test, net)\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch-------------{epoch}\")\n",
    "        print(f\"loss: {loss:>7f}\")\n",
    "        print(\n",
    "            f\"Test Error: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
